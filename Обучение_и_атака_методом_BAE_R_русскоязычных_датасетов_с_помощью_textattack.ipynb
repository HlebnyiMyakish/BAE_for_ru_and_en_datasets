{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e84g1YoseoE"
      },
      "source": [
        "# Обучение и атака методом BAE-R русскоязычных датасетов с помощью textattack\n",
        "\n",
        "Данный код был запущен в google collab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQTkpf9RslEA"
      },
      "outputs": [],
      "source": [
        "!pip3 install textattack[tensorflow]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-e9bzDX0OZ-"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak75xLKguyl5"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorflow-text==2.15.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu7-38FD4VAd"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li9THTl75yV5"
      },
      "outputs": [],
      "source": [
        "!pip install requests==2.27.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONayD5EJseoG"
      },
      "source": [
        "## Training\n",
        "\n",
        "First, we're going to train a model. TextAttack integrates directly with [transformers](https://github.com/huggingface/transformers/) and [datasets](https://github.com/huggingface/datasets) to train any of the `transformers` pre-trained models on datasets from `datasets`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMJoX4bifqEc"
      },
      "outputs": [],
      "source": [
        "dataset = [\"AlexSham/Toxic_Russian_Comments\", \"AlexSham/DaNetQA_for_BERT\", \"AlexSham/imdb_filtered\", 'AlexSham/TERRA_for_BERT',\n",
        "           \"Q-b1t/IMDB-Dataset-of-50K-Movie-Reviews-Backup\", 'AlexSham/RuTweetCorpMerged'][0] #--dataset_columns \"sentiment\",\"review\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spS2eW5WseoG",
        "outputId": "45da9f63-9363-49f5-bf28-5c70bcac2e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!textattack peek-dataset --dataset-from-huggingface \"AlexSham/Toxic_Russian_Comments\"\n"
          ]
        }
      ],
      "source": [
        "#!textattack peek-dataset --dataset imdb\n",
        "#!textattack peek-dataset --dataset-from-huggingface \"imdb\" #\"Q-b1t/IMDB-Dataset-of-50K-Movie-Reviews-Backup\" #--dataset_columns \"sentiment\",\"review\"\n",
        "import os\n",
        "print(f'!textattack peek-dataset --dataset-from-huggingface \"{dataset}\"')\n",
        "#os.system(f'textattack peek-dataset --dataset-from-huggingface \"{dataset}\"')\n",
        "#!textattack peek-dataset --dataset-from-huggingface f'{dataset}' #\"AlexSham/Toxic_Russian_Comments\" #\"AlexSham/DaNetQA_for_BERT\"#\"AlexSham/imdb_filtered\" #\"Q-b1t/IMDB-Dataset-of-50K-Movie-Reviews-Backup\" #--dataset_columns \"sentiment\",\"review\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVnY6BIDwmxA",
        "outputId": "518c6a37-1259-42f3-e2fc-7287439905dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
            "2024-04-12 16:08:48.221418: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-12 16:08:48.221507: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-12 16:08:48.223646: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-12 16:08:50.268894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading readme: 100% 1.07k/1.07k [00:00<00:00, 6.13MB/s]\n",
            "Downloading data: 100% 92.5M/92.5M [00:08<00:00, 11.1MB/s]\n",
            "Downloading data: 100% 10.3M/10.3M [00:01<00:00, 7.22MB/s]\n",
            "Generating train split: 223461 examples [00:00, 395701.02 examples/s]\n",
            "Generating test split: 24829 examples [00:00, 376723.72 examples/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mAlexSham/Toxic_Russian_Comments\u001b[0m, split \u001b[94mtrain\u001b[0m.\n",
            "Building prefix dict from the default dictionary ...\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.685 seconds.\n",
            "Loading model cost 0.685 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Prefix dict has been built successfully.\n",
            "\u001b[34;1mtextattack\u001b[0m: Number of samples: \u001b[94m223461\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: Number of words per input:\n",
            "\u001b[34;1mtextattack\u001b[0m: \ttotal:   \u001b[94m2833649\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tmean:    \u001b[94m12.68\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tstd:     \u001b[94m15.62\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tmin:     \u001b[94m1\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: \tmax:     \u001b[94m552\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: Dataset lowercased: \u001b[94mTrue\u001b[0m\n",
            "\u001b[34;1mtextattack\u001b[0m: First sample:\n",
            "видимо в разных регионах называют по разному , в ростовской области чаще называют шелковицей, реже тюте(и)ной или тутовником \n",
            "\n",
            "\u001b[34;1mtextattack\u001b[0m: Last sample:\n",
            "интересно чей это самолет! \n",
            "\n",
            "\u001b[34;1mtextattack\u001b[0m: Found 2 distinct outputs.\n",
            "\u001b[34;1mtextattack\u001b[0m: Most common outputs:\n",
            "\t 0      (183316)\n",
            "\t 1      (40145)\n"
          ]
        }
      ],
      "source": [
        "!textattack peek-dataset --dataset-from-huggingface \"AlexSham/Toxic_Russian_Comments\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3cmysh8_SE-"
      },
      "outputs": [],
      "source": [
        "!textattack train -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T8X1L8Ofclv",
        "outputId": "147c8ebc-960d-4160-f9c5-a49d71ef4e69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!textattack train --model-name-or-path \"DeepPavlov/distilrubert-tiny-cased-conversational-v1\" --dataset \"AlexSham/Toxic_Russian_Comments\" --model-num-labels 2 --model-max-length 512 --per-device-train-batch-size 4 --num-epochs 1\n"
          ]
        }
      ],
      "source": [
        "#DeepPavlov/distilrubert-tiny-cased-conversational-v1\n",
        "#ai-forever/ruBert-base\n",
        "model_path = ['ai-forever/ruBert-base', 'DeepPavlov/distilrubert-tiny-cased-conversational-v1', 'DeepPavlov/rubert-base-cased-conversational'][1]\n",
        "model_num_labels = 2\n",
        "model_max_length = 512\n",
        "per_device_train_bs = 4\n",
        "num_epochs = 1\n",
        "query = f'textattack train --model-name-or-path \"{model_path}\" --dataset \"{dataset}\" --model-num-labels {model_num_labels} --model-max-length {model_max_length} --per-device-train-batch-size {per_device_train_bs} --num-epochs {num_epochs}'\n",
        "print('!' + query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUN0utVvihxN",
        "outputId": "2c47491c-15eb-4839-d5e5-20a1798a17a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
            "2024-04-12 16:13:17.728632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-12 16:13:17.728674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-12 16:13:17.730008: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-12 16:13:18.805008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading transformers AutoModelForSequenceClassification: DeepPavlov/distilrubert-tiny-cased-conversational-v1\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/distilrubert-tiny-cased-conversational-v1 and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mAlexSham/Toxic_Russian_Comments\u001b[0m, split \u001b[94mtrain\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mAlexSham/Toxic_Russian_Comments\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Writing logs to ./outputs/2024-04-12-16-13-22-832465/train_log.txt.\n",
            "\u001b[34;1mtextattack\u001b[0m: Wrote original training args to ./outputs/2024-04-12-16-13-22-832465/training_args.json.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34;1mtextattack\u001b[0m: ***** Running training *****\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num examples = 223461\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num epochs = 1\n",
            "\u001b[34;1mtextattack\u001b[0m:   Num clean epochs = 1\n",
            "\u001b[34;1mtextattack\u001b[0m:   Instantaneous batch size per device = 4\n",
            "\u001b[34;1mtextattack\u001b[0m:   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "\u001b[34;1mtextattack\u001b[0m:   Gradient accumulation steps = 1\n",
            "\u001b[34;1mtextattack\u001b[0m:   Total optimization steps = 55866\n",
            "\u001b[34;1mtextattack\u001b[0m: ==========================================================\n",
            "\u001b[34;1mtextattack\u001b[0m: Epoch 1\n",
            "\u001b[34;1mtextattack\u001b[0m: Running clean epoch 1/1\n",
            "Loss 0.08654: 100% 55866/55866 [34:37<00:00, 26.89it/s]\n",
            "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 96.82%\n",
            "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 97.85%\n",
            "\u001b[34;1mtextattack\u001b[0m: Best score found. Saved model to ./outputs/2024-04-12-16-13-22-832465/best_model/\n",
            "\u001b[34;1mtextattack\u001b[0m: Wrote README to ./outputs/2024-04-12-16-13-22-832465/README.md.\n"
          ]
        }
      ],
      "source": [
        "!textattack train --model-name-or-path \"DeepPavlov/distilrubert-tiny-cased-conversational-v1\" --dataset \"AlexSham/Toxic_Russian_Comments\" --model-num-labels 2 --model-max-length 512 --per-device-train-batch-size 4 --num-epochs 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY33W9aWseoI"
      },
      "outputs": [],
      "source": [
        "#os.system(f'textattack train --model-name-or-path \"{model_path}\" --dataset \"{dataset}\" --model-num-labels {model_num_labels} --model-max-length {model_max_length} --per-device-train-batch-size {per_device_train_bs} --num-epochs {num_epochs}')\n",
        "#import subprocess\n",
        "#output = subprocess.check_output(query, shell=True)\n",
        "#print(output)\n",
        "#!textattack train --model-name-or-path \"ai-forever/ruBert-base\" --dataset \"AlexSham/Toxic_Russian_Comments\" --model-num-labels 2 --model-max-length 256 --per-device-train-batch-size 256 --num-epochs 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xzv3BGLseoI"
      },
      "source": [
        "## Оценка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PogAR2-pCnMJ",
        "outputId": "1d0fe268-1870-42e9-81ec-544ddc114c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!textattack eval --num-examples 100 --model /content/outputs/2024-04-12-16-13-22-832465/best_model/ --dataset-from-huggingface \"AlexSham/Toxic_Russian_Comments\" --dataset-split test\n"
          ]
        }
      ],
      "source": [
        "date = '2024-04-12-16-13-22-832465'\n",
        "\n",
        "query = f'!textattack eval --num-examples 100 --model /content/outputs/{date}/best_model/ --dataset-from-huggingface \"{dataset}\" --dataset-split test'\n",
        "print(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGYR_W6DseoJ",
        "outputId": "1eaebde6-2349-4ce0-8e81-753c08bd5f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
            "2024-04-12 16:59:19.409215: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-12 16:59:19.409271: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-12 16:59:19.410699: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-12 16:59:20.596839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mAlexSham/Toxic_Russian_Comments\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
            "\u001b[34;1mtextattack\u001b[0m: Got 100 predictions.\n",
            "\u001b[34;1mtextattack\u001b[0m: Correct 100/100 (\u001b[94m100.00%\u001b[0m)\n"
          ]
        }
      ],
      "source": [
        "!textattack eval --num-examples 100 --model /content/outputs/2024-04-12-16-13-22-832465/best_model/ --dataset-from-huggingface \"AlexSham/Toxic_Russian_Comments\" --dataset-split test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWglEuvUseoK"
      },
      "source": [
        "## Атака\n",
        "Атакуем нашу лучшую модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fnSUl8ND9u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924eb6fc-0827-48b1-e036-d053252254bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
          ]
        }
      ],
      "source": [
        "#from textattack.attack_recipes import BAEGarg2019\n",
        "from textattack.datasets import HuggingFaceDataset\n",
        "from textattack.models.wrappers import ModelWrapper\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, TFAutoModelForSequenceClassification, AutoModelForSequenceClassification, pipeline, BertModel\n",
        "from textattack import Attacker\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Quiet TensorFlow.\n",
        "import os\n",
        "\n",
        "if \"TF_CPP_MIN_LOG_LEVEL\" not in os.environ:\n",
        "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "\n",
        "class HuggingFaceSentimentAnalysisPipelineWrapper(ModelWrapper):\n",
        "    \"\"\"Transformers sentiment analysis pipeline returns a list of responses\n",
        "    like\n",
        "\n",
        "        [{'label': 'POSITIVE', 'score': 0.7817379832267761}]\n",
        "\n",
        "    We need to convert that to a format TextAttack understands, like\n",
        "\n",
        "        [[0.218262017, 0.7817379832267761]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model  # pipeline = pipeline\n",
        "\n",
        "    def __call__(self, text_inputs):\n",
        "        raw_outputs = self.model(text_inputs)\n",
        "        outputs = []\n",
        "        for output in raw_outputs:\n",
        "            score = output[\"score\"]\n",
        "            if output[\"label\"] == \"LABEL_0\":\n",
        "                outputs.append([score, 1-score])\n",
        "            else:\n",
        "                outputs.append([1-score, score])\n",
        "        return torch.tensor(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nWjx-ByE3Hf",
        "outputId": "c3e0c9fb-a43c-4cf9-e3cc-92b567822e67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'label': 'LABEL_0', 'score': 0.9997534155845642}],\n",
              " tensor([[9.9975e-01, 2.4658e-04]]),\n",
              " [{'label': 'LABEL_1', 'score': 0.9402301907539368}],\n",
              " tensor([[0.0598, 0.9402]]),\n",
              " [{'label': 'LABEL_1', 'score': 0.9989711046218872}],\n",
              " tensor([[0.0010, 0.9990]]),\n",
              " [{'label': 'LABEL_0', 'score': 0.9978470802307129}],\n",
              " tensor([[0.9978, 0.0022]]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model_date = date#'2024-04-12-06-21-48-570416'\n",
        "model_path = f'/content/outputs/{model_date}/best_model/'\n",
        "ds_name = dataset#['RussianNLP/russian_super_glue', 'AlexSham/Toxic_Russian_Comments', 's-nlp/ru_paradetox_toxicity', 'AlexSham/DaNetQA_for_BERT', 'AlexSham/RuTweetCorpMerged'][-1]\n",
        "split = ['train', 'test'][1]\n",
        "pipe = pipeline(\"text-classification\", model=model_path, tokenizer=model_path,  framework=\"pt\")\n",
        "model_wrapper = HuggingFaceSentimentAnalysisPipelineWrapper(pipe)\n",
        "pipe('ты умный)'), model_wrapper('ты умный)'), pipe('Тупой дурак'), model_wrapper('Тупой дурак'), pipe('блин'), model_wrapper('блин'), pipe('Женщину доставили в больницу, за ее жизнь сейчас борются врачи. [SEP] Женщину спасают врачи'), model_wrapper('Женщину доставили в больницу, за ее жизнь сейчас борются врачи. [SEP] Женщину спасают врачи')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drdQd_cINHkk"
      },
      "outputs": [],
      "source": [
        "from textattack.constraints import Constraint\n",
        "import re\n",
        "\n",
        "class NoServiceTokens(Constraint):\n",
        "  \"\"\"A constraint that ensures `transformed_text` only substitutes named entities from `current_text` with other named entities.\"\"\"\n",
        "\n",
        "  def _check_constraint(self, transformed_text, current_text):\n",
        "    regex_service = re.compile('\\[unused[0-9]+\\]|\\[PAD\\]|\\[SEP\\]')\n",
        "    #print(transformed_text.text)\n",
        "    if regex_service.match(transformed_text.text) == regex_service.match(current_text.text):\n",
        "      return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFCaypACENom"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Word Swap by BERT-Masked LM.\n",
        "-------------------------------\n",
        "\"\"\"\n",
        "\n",
        "import itertools\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "from textattack.shared import utils\n",
        "\n",
        "from textattack.transformations.word_swaps.word_swap import WordSwap\n",
        "\n",
        "\n",
        "class WordSwapMaskedLM(WordSwap):\n",
        "    \"\"\"Generate potential replacements for a word using a masked language\n",
        "    model.\n",
        "\n",
        "    Based off of following papers\n",
        "        - \"Robustness to Modification with Shared Words in Paraphrase Identification\" (Shi et al., 2019) https://arxiv.org/abs/1909.02560\n",
        "        - \"BAE: BERT-based Adversarial Examples for Text Classification\" (Garg et al., 2020) https://arxiv.org/abs/2004.01970\n",
        "        - \"BERT-ATTACK: Adversarial Attack Against BERT Using BERT\" (Li et al, 2020) https://arxiv.org/abs/2004.09984\n",
        "        - \"CLARE: Contextualized Perturbation for Textual Adversarial Attack\" (Li et al, 2020): https://arxiv.org/abs/2009.07502\n",
        "\n",
        "    BAE and CLARE simply masks the word we want to replace and selects replacements predicted by the masked language model.\n",
        "\n",
        "    BERT-Attack instead performs replacement on token level. For words that are consisted of two or more sub-word tokens,\n",
        "        it takes the top-K replacements for seach sub-word token and produces all possible combinations of the top replacments.\n",
        "        Then, it selects the top-K combinations based on their perplexity calculated using the masked language model.\n",
        "\n",
        "    Choose which method to use by specifying \"bae\" or \"bert-attack\" for `method` argument.\n",
        "\n",
        "    Args:\n",
        "        method (str): the name of replacement method (e.g. \"bae\", \"bert-attack\")\n",
        "        masked_language_model (Union[str|transformers.AutoModelForMaskedLM]): Either the name of pretrained masked language model from `transformers` model hub\n",
        "            or the actual model. Default is `bert-base-uncased`.\n",
        "        tokenizer (obj): The tokenizer of the corresponding model. If you passed in name of a pretrained model for `masked_language_model`,\n",
        "            you can skip this argument as the correct tokenizer can be infered from the name. However, if you're passing the actual model, you must\n",
        "            provide a tokenizer.\n",
        "        max_length (int): the max sequence length the masked language model is designed to work with. Default is 512.\n",
        "        window_size (int): The number of surrounding words to include when making top word prediction.\n",
        "            For each word to swap, we take `window_size // 2` words to the left and `window_size // 2` words to the right and pass the text within the window\n",
        "            to the masked language model. Default is `float(\"inf\")`, which is equivalent to using the whole text.\n",
        "        max_candidates (int): maximum number of candidates to consider as replacements for each word. Replacements are ranked by model's confidence.\n",
        "        min_confidence (float): minimum confidence threshold each replacement word must pass.\n",
        "        batch_size (int): Size of batch for \"bae\" replacement method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        method=\"bae\",\n",
        "        masked_language_model=\"bert-base-uncased\",\n",
        "        tokenizer=None,\n",
        "        max_length=512,\n",
        "        window_size=float(\"inf\"),\n",
        "        max_candidates=50,\n",
        "        min_confidence=5e-4,\n",
        "        batch_size=16,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.method = method\n",
        "        self.max_length = max_length\n",
        "        self.window_size = window_size\n",
        "        self.max_candidates = max_candidates\n",
        "        self.min_confidence = min_confidence\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        if isinstance(masked_language_model, str):\n",
        "            self._language_model = AutoModelForMaskedLM.from_pretrained(\n",
        "                masked_language_model\n",
        "            )\n",
        "            self._lm_tokenizer = AutoTokenizer.from_pretrained(\n",
        "                masked_language_model, use_fast=True\n",
        "            )\n",
        "        else:\n",
        "            self._language_model = masked_language_model\n",
        "            if tokenizer is None:\n",
        "                raise ValueError(\n",
        "                    \"`tokenizer` argument must be provided when passing an actual model as `masked_language_model`.\"\n",
        "                )\n",
        "            self._lm_tokenizer = tokenizer\n",
        "        self._language_model.to(utils.device)\n",
        "        self._language_model.eval()\n",
        "        self.masked_lm_name = self._language_model.__class__.__name__\n",
        "\n",
        "    def _encode_text(self, text):\n",
        "        \"\"\"Encodes ``text`` using an ``AutoTokenizer``, ``self._lm_tokenizer``.\n",
        "\n",
        "        Returns a ``dict`` where keys are strings (like 'input_ids') and\n",
        "        values are ``torch.Tensor``s. Moves tensors to the same device\n",
        "        as the language model.\n",
        "        \"\"\"\n",
        "        encoding = self._lm_tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return encoding.to(utils.device)\n",
        "\n",
        "    def _bae_replacement_words(self, current_text, indices_to_modify):\n",
        "        \"\"\"Get replacement words for the word we want to replace using BAE\n",
        "        method.\n",
        "\n",
        "        Args:\n",
        "            current_text (AttackedText): Text we want to get replacements for.\n",
        "            index (int): index of word we want to replace\n",
        "        \"\"\"\n",
        "        masked_texts = []\n",
        "        for index in indices_to_modify:\n",
        "            masked_text = current_text.replace_word_at_index(\n",
        "                index, self._lm_tokenizer.mask_token\n",
        "            )\n",
        "            masked_texts.append(masked_text.text)\n",
        "\n",
        "        i = 0\n",
        "        # 2-D list where for each index to modify we have a list of replacement words\n",
        "        replacement_words = []\n",
        "        while i < len(masked_texts):\n",
        "            inputs = self._encode_text(masked_texts[i : i + self.batch_size])\n",
        "            ids = inputs[\"input_ids\"].tolist()\n",
        "            with torch.no_grad():\n",
        "                preds = self._language_model(**inputs)[0]\n",
        "\n",
        "            for j in range(len(ids)):\n",
        "                try:\n",
        "                    # Need try-except b/c mask-token located past max_length might be truncated by tokenizer\n",
        "                    masked_index = ids[j].index(self._lm_tokenizer.mask_token_id)\n",
        "                except ValueError:\n",
        "                    replacement_words.append([])\n",
        "                    continue\n",
        "\n",
        "                mask_token_logits = preds[j, masked_index]\n",
        "                mask_token_probs = torch.softmax(mask_token_logits, dim=0)\n",
        "                ranked_indices = torch.argsort(mask_token_probs, descending=True)\n",
        "                top_words = []\n",
        "                for _id in ranked_indices:\n",
        "                    _id = _id.item()\n",
        "                    word = self._lm_tokenizer.convert_ids_to_tokens(_id)\n",
        "                    if utils.check_if_subword(\n",
        "                        word,\n",
        "                        self._language_model.config.model_type,\n",
        "                        (masked_index == 1),\n",
        "                    ):\n",
        "                        word = utils.strip_BPE_artifacts(\n",
        "                            word, self._language_model.config.model_type\n",
        "                        )\n",
        "                    if (\n",
        "                        mask_token_probs[_id] >= self.min_confidence\n",
        "                        and utils.is_one_word(word)\n",
        "                        and not utils.check_if_punctuations(word)\n",
        "                    ):\n",
        "                        top_words.append(word)\n",
        "\n",
        "                    if (\n",
        "                        len(top_words) >= self.max_candidates\n",
        "                        or mask_token_probs[_id] < self.min_confidence\n",
        "                    ):\n",
        "                        break\n",
        "\n",
        "                replacement_words.append(top_words)\n",
        "\n",
        "            i += self.batch_size\n",
        "\n",
        "        return replacement_words\n",
        "\n",
        "    def _bert_attack_replacement_words(\n",
        "        self,\n",
        "        current_text,\n",
        "        index,\n",
        "        id_preds,\n",
        "        masked_lm_logits,\n",
        "    ):\n",
        "        \"\"\"Get replacement words for the word we want to replace using BERT-\n",
        "        Attack method.\n",
        "\n",
        "        Args:\n",
        "            current_text (AttackedText): Text we want to get replacements for.\n",
        "            index (int): index of word we want to replace\n",
        "            id_preds (torch.Tensor): N x K tensor of top-K ids for each token-position predicted by the masked language model.\n",
        "                N is equivalent to `self.max_length`.\n",
        "            masked_lm_logits (torch.Tensor): N x V tensor of the raw logits outputted by the masked language model.\n",
        "                N is equivlaent to `self.max_length` and V is dictionary size of masked language model.\n",
        "        \"\"\"\n",
        "        # We need to find which BPE tokens belong to the word we want to replace\n",
        "        masked_text = current_text.replace_word_at_index(\n",
        "            index, self._lm_tokenizer.mask_token\n",
        "        )\n",
        "        current_inputs = self._encode_text(masked_text.text)\n",
        "        current_ids = current_inputs[\"input_ids\"].tolist()[0]\n",
        "        word_tokens = self._lm_tokenizer.encode(\n",
        "            current_text.words[index], add_special_tokens=False\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Need try-except b/c mask-token located past max_length might be truncated by tokenizer\n",
        "            masked_index = current_ids.index(self._lm_tokenizer.mask_token_id)\n",
        "        except ValueError:\n",
        "            return []\n",
        "\n",
        "        # List of indices of tokens that are part of the target word\n",
        "        target_ids_pos = list(\n",
        "            range(masked_index, min(masked_index + len(word_tokens), self.max_length))\n",
        "        )\n",
        "\n",
        "        if not len(target_ids_pos):\n",
        "            return []\n",
        "        elif len(target_ids_pos) == 1:\n",
        "            # Word to replace is tokenized as a single word\n",
        "            top_preds = id_preds[target_ids_pos[0]].tolist()\n",
        "            replacement_words = []\n",
        "            for id in top_preds:\n",
        "                token = self._lm_tokenizer.convert_ids_to_tokens(id)\n",
        "                if utils.is_one_word(token) and not utils.check_if_subword(\n",
        "                    token, self._language_model.config.model_type, index == 0\n",
        "                ):\n",
        "                    replacement_words.append(token)\n",
        "            return replacement_words\n",
        "        else:\n",
        "            # Word to replace is tokenized as multiple sub-words\n",
        "            top_preds = [id_preds[i] for i in target_ids_pos]\n",
        "            products = itertools.product(*top_preds)\n",
        "            combination_results = []\n",
        "            # Original BERT-Attack implement uses cross-entropy loss\n",
        "            cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
        "            target_ids_pos_tensor = torch.tensor(target_ids_pos)\n",
        "            word_tensor = torch.zeros(len(target_ids_pos), dtype=torch.long)\n",
        "            for bpe_tokens in products:\n",
        "                for i in range(len(bpe_tokens)):\n",
        "                    word_tensor[i] = bpe_tokens[i]\n",
        "\n",
        "                logits = torch.index_select(masked_lm_logits, 0, target_ids_pos_tensor)\n",
        "                loss = cross_entropy_loss(logits, word_tensor)\n",
        "                perplexity = torch.exp(torch.mean(loss, dim=0)).item()\n",
        "                word = \"\".join(\n",
        "                    self._lm_tokenizer.convert_ids_to_tokens(word_tensor)\n",
        "                ).replace(\"##\", \"\")\n",
        "                if utils.is_one_word(word):\n",
        "                    combination_results.append((word, perplexity))\n",
        "            # Sort to get top-K results\n",
        "            sorted(combination_results, key=lambda x: x[1])\n",
        "            top_replacements = [\n",
        "                x[0] for x in combination_results[: self.max_candidates]\n",
        "            ]\n",
        "            return top_replacements\n",
        "\n",
        "    def _get_transformations(self, current_text, indices_to_modify):\n",
        "        indices_to_modify = list(indices_to_modify)\n",
        "        if self.method == \"bert-attack\":\n",
        "            current_inputs = self._encode_text(current_text.text)\n",
        "            with torch.no_grad():\n",
        "                pred_probs = self._language_model(**current_inputs)[0][0]\n",
        "            top_probs, top_ids = torch.topk(pred_probs, self.max_candidates)\n",
        "            id_preds = top_ids.cpu()\n",
        "            masked_lm_logits = pred_probs.cpu()\n",
        "\n",
        "            transformed_texts = []\n",
        "\n",
        "            for i in indices_to_modify:\n",
        "                word_at_index = current_text.words[i]\n",
        "                replacement_words = self._bert_attack_replacement_words(\n",
        "                    current_text,\n",
        "                    i,\n",
        "                    id_preds=id_preds,\n",
        "                    masked_lm_logits=masked_lm_logits,\n",
        "                )\n",
        "\n",
        "                for r in replacement_words:\n",
        "                    r = r.strip(\"Ġ\")\n",
        "                    if r != word_at_index:\n",
        "                        transformed_texts.append(\n",
        "                            current_text.replace_word_at_index(i, r)\n",
        "                        )\n",
        "\n",
        "            return transformed_texts\n",
        "\n",
        "        elif self.method == \"bae\":\n",
        "            replacement_words = self._bae_replacement_words(\n",
        "                current_text, indices_to_modify\n",
        "            )\n",
        "            transformed_texts = []\n",
        "            for i in range(len(replacement_words)):\n",
        "                index_to_modify = indices_to_modify[i]\n",
        "                word_at_index = current_text.words[index_to_modify]\n",
        "                for word in replacement_words[i]:\n",
        "                    #word = word.strip(\"Ġ\")\n",
        "                    if (\n",
        "                        word != word_at_index\n",
        "                        #and re.search(\"[a-zA-Z]\", word)\n",
        "                        #and len(utils.words_from_text(word)) == 1\n",
        "                    ):\n",
        "                        transformed_texts.append(\n",
        "                            current_text.replace_word_at_index(index_to_modify, word)\n",
        "                        )\n",
        "            return transformed_texts\n",
        "        else:\n",
        "            raise ValueError(f\"Unrecognized value {self.method} for `self.method`.\")\n",
        "\n",
        "    def extra_repr_keys(self):\n",
        "        return [\n",
        "            \"method\",\n",
        "            \"masked_lm_name\",\n",
        "            \"max_length\",\n",
        "            \"max_candidates\",\n",
        "            \"min_confidence\",\n",
        "        ]\n",
        "\n",
        "\n",
        "def recover_word_case(word, reference_word):\n",
        "    \"\"\"Makes the case of `word` like the case of `reference_word`.\n",
        "\n",
        "    Supports lowercase, UPPERCASE, and Capitalized.\n",
        "    \"\"\"\n",
        "    if reference_word.islower():\n",
        "        return word.lower()\n",
        "    elif reference_word.isupper() and len(reference_word) > 1:\n",
        "        return word.upper()\n",
        "    elif reference_word[0].isupper() and reference_word[1:].islower():\n",
        "        return word.capitalize()\n",
        "    else:\n",
        "        # if other, just do not alter the word's case\n",
        "        return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAxI6ha-mrnV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "BAE (BAE: BERT-Based Adversarial Examples)\n",
        "============================================\n",
        "added GoogLMHelper constraint\n",
        "\"\"\"\n",
        "from textattack.constraints.grammaticality.language_models.google_language_model.alzantot_goog_lm import GoogLMHelper\n",
        "from textattack.constraints.grammaticality import PartOfSpeech\n",
        "from textattack.constraints.pre_transformation import (\n",
        "    RepeatModification,\n",
        "    StopwordModification,\n",
        ")\n",
        "from textattack.constraints.semantics.sentence_encoders.universal_sentence_encoder.multilingual_universal_sentence_encoder import MultilingualUniversalSentenceEncoder\n",
        "from textattack.goal_functions import UntargetedClassification\n",
        "from textattack.search_methods import GreedyWordSwapWIR\n",
        "\n",
        "from textattack.attack_recipes.attack_recipe import AttackRecipe\n",
        "\n",
        "class BAEGarg2019(AttackRecipe):\n",
        "    \"\"\"Siddhant Garg and Goutham Ramakrishnan, 2019.\n",
        "\n",
        "    BAE: BERT-based Adversarial Examples for Text Classification.\n",
        "\n",
        "    https://arxiv.org/pdf/2004.01970\n",
        "\n",
        "    This is \"attack mode\" 1 from the paper, BAE-R, word replacement.\n",
        "\n",
        "    We present 4 attack modes for BAE based on the\n",
        "        R and I operations, where for each token t in S:\n",
        "        • BAE-R: Replace token t (See Algorithm 1)\n",
        "        • BAE-I: Insert a token to the left or right of t\n",
        "        • BAE-R/I: Either replace token t or insert a\n",
        "        token to the left or right of t\n",
        "        • BAE-R+I: First replace token t, then insert a\n",
        "        token to the left or right of t\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def build(model_wrapper):\n",
        "        model_name = ['DeepPavlov/rubert-base-cased-conversational','ai-forever/ruBert-base'][1]#\n",
        "        model = AutoModelForMaskedLM.from_pretrained(model_name)#, from_pt=True)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "        transformation = WordSwapMaskedLM(\n",
        "            method=\"bae\", max_candidates=50, min_confidence=0.0, masked_language_model = model, tokenizer = tokenizer\n",
        "        )\n",
        "        #\n",
        "        # Don't modify the same word twice or stopwords.\n",
        "        #\n",
        "        constraints = [RepeatModification(), StopwordModification(language='russian')]\n",
        "        constraints.append(NoServiceTokens(True))\n",
        "        #constraints.append(PartOfSpeech(allow_verb_noun_swap=True))#, language_nltk=\"rus\"))\n",
        "        use_constraint = MultilingualUniversalSentenceEncoder(\n",
        "            threshold=0.936338023,\n",
        "            metric=\"cosine\",\n",
        "            compare_against_original=True,\n",
        "            window_size=15,\n",
        "            skip_text_shorter_than_window=False,#True\n",
        "        )\n",
        "        constraints.append(use_constraint)\n",
        "        #googlm_constraint = GoogLMHelper\n",
        "        #\n",
        "        # Goal is untargeted classification.\n",
        "        #\n",
        "        goal_function = UntargetedClassification(model_wrapper)\n",
        "        #\n",
        "        # \"We estimate the token importance Ii of each token\n",
        "        # t_i ∈ S = [t1, . . . , tn], by deleting ti from S and computing the\n",
        "        # decrease in probability of predicting the correct label y, similar\n",
        "        # to (Jin et al., 2019).\n",
        "        #\n",
        "        # • \"If there are multiple tokens can cause C to misclassify S when they\n",
        "        # replace the mask, we choose the token which makes Sadv most similar to\n",
        "        # the original S based on the USE score.\"\n",
        "        # • \"If no token causes misclassification, we choose the perturbation that\n",
        "        # decreases the prediction probability P(C(Sadv)=y) the most.\"\n",
        "        #\n",
        "        search_method = GreedyWordSwapWIR(wir_method=\"delete\")\n",
        "\n",
        "        return BAEGarg2019(goal_function, constraints, transformation, search_method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJFjmiEemnka"
      },
      "outputs": [],
      "source": [
        "model_date = date#'2024-04-11-15-30-58-482536'\n",
        "model_path = f'/content/outputs/{model_date}/best_model/'\n",
        "ds_name = dataset#['RussianNLP/russian_super_glue', 'AlexSham/Toxic_Russian_Comments', 's-nlp/ru_paradetox_toxicity', 'AlexSham/DaNetQA_for_BERT', 'AlexSham/RuTweetCorpMerged'][-1]\n",
        "split = ['train', 'test'][1]\n",
        "pipe = pipeline(\"text-classification\", model=model_path, tokenizer=model_path,  framework=\"pt\")\n",
        "model_wrapper = HuggingFaceSentimentAnalysisPipelineWrapper(pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2WPtwO9D9u6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "path = model_path #'/content/outputs/2024-04-04-15-36-26-801838/best_model'#'IlyaGusev/rubertconv_toxic_clf'#'seninoseno/rubert-base-cased-sentiment-study-feedbacks-solyanka' #'sismetanin/rubert-ru-sentiment-rusentiment'\n",
        "pipe = pipeline(\"text-classification\", model=path, tokenizer=path,  framework=\"pt\")\n",
        "\n",
        "\n",
        "\n",
        "model_wrapper = HuggingFaceSentimentAnalysisPipelineWrapper(pipe)\n",
        "\n",
        "\n",
        "from textattack.attack_recipes.textfooler_jin_2019 import TextFoolerJin2019\n",
        "# Create the recipe: PWWS uses a WordNet transformation.\n",
        "recipe = BAEGarg2019.build(model_wrapper)\n",
        "#recipe = TextFoolerJin2019.build(model_wrapper)\n",
        "recipe.transformation.language = \"rus\"\n",
        "\n",
        "name = ds_name #['RussianNLP/russian_super_glue', 'AlexSham/Toxic_Russian_Comments', 's-nlp/ru_paradetox_toxicity', 'AlexSham/DaNetQA_for_BERT'][-1]\n",
        "\n",
        "dataset = HuggingFaceDataset(name, split=split)#HuggingFaceDataset(\"s-nlp/ru_paradetox_toxicity\", split=\"train\", dataset_columns = [['neutral'], 'toxic'])\n",
        "#dataset_columns=['text', 'positive']\n",
        "\n",
        "import textattack\n",
        "attack_args = textattack.AttackArgs(\n",
        "     num_examples=100,\n",
        "     log_to_csv=\"log.csv\",\n",
        "     checkpoint_interval=5,\n",
        "     checkpoint_dir=\"checkpoints\"\n",
        " )\n",
        "\n",
        "attacker = Attacker(recipe, dataset,attack_args)\n",
        "import nltk\n",
        "#nltk.download('averaged_perceptron_tagger_ru')\n",
        "#nltk.download()#'universal_tagset')\n",
        "attacker.attack_dataset()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}